# Day 05 Capture — Podcast Notes

**Date:** 2025-10-05  
**Source:** "AI Alignment Podcast" Episode 147  
**Guest:** Dr. Sarah Chen (AI Safety Researcher)  
**Type:** Rough listening notes

---

## Key Points (Paraphrased)

### On Risk Assessment
- "Most orgs treat AI risk as binary (safe/unsafe). Reality is a spectrum."
- Risk changes over time (model drift, data distribution shifts, environment changes)
- Need continuous risk assessment, not one-time review at deployment

### On Transparency
- "Transparency isn't just logging—it's making the logs interpretable"
- Stakeholders need different views: engineers see technical metrics, managers see business impact, users see decisions affecting them
- Built a "transparency dashboard" with role-based views

### On Human Oversight
- Not all decisions need same level of oversight
- Low-risk, high-volume → automated with periodic audits
- High-risk, low-volume → human approval required
- Hybrid: AI suggests, human decides (preserves agency)

---

## Quotes (Verbatim)

> "If you can't roll it back in under 5 minutes, you haven't deployed it—you've released it into the wild and hoped for the best."

> "Governance isn't red tape. It's the scaffolding that lets you move fast without breaking things that matter."

---

## Personal Thoughts

- Risk spectrum (not binary) resonates
- Transparency dashboard idea (different views for different roles)
- 5-minute rollback rule (concrete, measurable)
- Governance as enabling (not blocking)

---

**Status:** Rough notes, not cleaned up
